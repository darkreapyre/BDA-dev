---
- hosts: all
  tasks:
    - name: Create Admins group
      group: name=admin state=present
      sudo: yes

    - name: Create Admin user on all nodes
      user: name={{ cluster_user }}
            comment="Cluster Administrator"
            group={{ cluster_user }}
            createhome=yes
            home=/home/{{ cluster_user }}
            shell=/bin/bash
            password={{ cluster_password |password_hash('sha512') }}
            generate_ssh_key=yes
            ssh_key_bits=2048
            state=present
      sudo: yes

    - name: Add Admin user to sudoers
      lineinfile: "dest=/etc/sudoers insertafter=EOF line='{{ cluster_user }} ALL=(ALL) NOPASSWD: ALL' regexp='{{ cluster_user }} ALL=(ALL) NOPASSWD: ALL' state=present"
      sudo: yes

    - name: Ensure Admin user ssh directory exists
      file: dest=/home/{{ cluster_user }}/.ssh owner={{ cluster_user }} group={{ cluster_user }} state=directory mode=0700
      sudo: yes
      
    - name: Disable strict host checking for the Admin user
      lineinfile: create=yes
                  dest=/home/{{ cluster_user }}/.ssh/config
                  regexp=StrictHostKeyChecking
                  line="StrictHostKeyChecking no"
                  owner={{ cluster_user }}
                  group={{ cluster_user }}
                  mode=0644
      sudo: yes
# Issues with Permission on temporary files
#      become: yes
#      become_user: "{{ cluster_user }}"

    - name: Build hosts file on all servers
      lineinfile: dest=/etc/hosts regexp='.*{{ item }}$' line="{{ hostvars[item].ansible_default_ipv4.address }} {{item}}" state=present
      when: hostvars[item].ansible_default_ipv4.address is defined
      with_items: groups['all']
      sudo: yes
    
- hosts: master
  tasks:
    - name: Setup passwordless ssh
      shell: /bin/cat /home/{{ cluster_user }}/.ssh/id_rsa.pub >> /home/{{ cluster_user }}/.ssh/authorized_keys
      become: yes
      become_user: "{{ cluster_user }}"
    
    # Use the following for one time application downloads
    - name: Create local Appliaction Repository
      local_action: file path=apps state=directory
    
    - name: Download Hadoop locally
      local_action: get_url url=http://apache.claz.org/hadoop/common/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz dest=apps/hadoop-{{ hadoop_version }}.tar.gz
    
    - name: Download Spark locally
      local_action: get_url url=http://apache.claz.org/spark/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-hadoop2.6.tgz dest=apps/spark-{{ spark_version }}-bin-hadoop2.6.tgz
    
    - name: Download apache maven
      local_action: get_url url=http://mirror.nbtelecom.com.br/apache/maven/maven-3/{{ maven_version }}/binaries/apache-maven-{{ maven_version }}-bin.tar.gz dest=apps/apache-maven-{{ maven_version }}-bin.tar.gz
    
    - name: Download scala
      local_action: get_url url=http://downloads.typesafe.com/scala/{{ scala_version }}/scala-{{ scala_version }}.deb?_ga=1.129140876.1867818617.1456337141 dest=apps/scala-{{ scala_version }}.deb

    # The following is not necessary because "unarchive" has the "copy: yes" parameter
    #- name: Distribute downloaded files to slaves
    #  copy: src=apps/{{ item }} dest=/home/{{ cluster_user }}/{{ item }}
    #  with_items:
    #    - hadoop-{{ hadoop_version }}.tar.gz
    #    - spark-{{ spark_version }}-bin-hadoop2.6.tgz
    #    - apache-maven-{{ maven_version }}-bin.tar.gz
    #    - scala-{{ scala_version }}.deb
    #  become: yes
    #  become_user: "{{ cluster_user }}"

    # Distribute the Scala file to the master since it's a binary package
    - name: Distribute Scala to master
      copy: src=apps/scala-{{ scala_version }}.deb dest=/home/{{ cluster_user }}/scala-{{ scala_version }}.deb
      become: yes
      become_user: "{{ cluster_user }}"
      
    - name: Create local directory for ssh keys
      local_action: file path=sshkeys state=directory
    
    - name: Fetch ssh keys from the master
      fetch: src=/home/{{ cluster_user }}/.ssh/{{ item }} dest=sshkeys/{{ item }} flat=yes
      with_items:
        - id_rsa
        - id_rsa.pub
        - authorized_keys
      become: yes
      become_user: "{{ cluster_user }}"

- hosts: slaves:cassandra
  tasks:
    - name: Distribute ssh keys
      copy: src=sshkeys/{{ item }} dest=/home/{{ cluster_user }}/.ssh/{{ item }}
      with_items:
        - id_rsa
        - id_rsa.pub
        - authorized_keys
      become: yes
      become_user: "{{ cluster_user }}"
    
    # The following is not necessary because "unarchive" has the "copy: yes" parameter
    #- name: Distribute downloaded files to slaves
    #  copy: src=apps/{{ item }} dest=/home/{{ cluster_user }}/{{ item }}
    #  with_items:
    #    - hadoop-{{ hadoop_version }}.tar.gz
    #    - spark-{{ spark_version }}-bin-hadoop2.6.tgz
    #    - apache-maven-{{ maven_version }}-bin.tar.gz
    #    - scala-{{ scala_version }}.deb
    #  become: yes
    #  become_user: "{{ cluster_user }}"
    
    # Distribute the Scala file to the master since it's a binary package
    - name: Distribute Scala
      copy: src=apps/scala-{{ scala_version }}.deb dest=/home/{{ cluster_user }}/scala-{{ scala_version }}.deb
      become: yes
      become_user: "{{ cluster_user }}"
    
    - name: Ensure correct permissions on the ssh keys
      file: path=/home/{{ cluster_user }}/.ssh/{{ item }} mode=0600 owner={{ cluster_user }} group={{ cluster_user }}
      with_items:
        - id_rsa
        - id_rsa.pub
        - authorized_keys
      become: yes
      become_user: "{{ cluster_user }}"

- name: Install base confiuration on all Spark/Hadoop servers
  hosts: all:!cassandra
  remote_user: vagrant
  sudo: yes

  roles:
    - common

- name: Install hadoop on all Spark/Hadoop servers
  hosts: all:!cassandra
  remote_user: vagrant
  sudo: yes
  
  roles:
    - hadoop

- name: Install spark on all Spark/Hadoop servers
  hosts: all:!cassandra
  remote_user: vagrant
  sudo: yes
  
  roles:
    - spark

- name: Install Cassandra servers
  hosts: cassandra
  remote_user: vagrant
  sudo: yes

  roles:
    - cassandra_install

- name: Configure Data Science Libraries on the Spark Master
  hosts: master
  remote_user: vagrant
  sudo: yes
  
  roles:
    - dsbox

- name: Configure Cassandra Cluster
  hosts: cassandra
  remote_user: vagrant
  sudo: yes

  roles:
    - cassandra_cluster