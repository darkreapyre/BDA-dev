{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "val conf = new SparkConf().set(\"spark.cassandra.connection.host\", \"master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sc = new SparkContext(\"spark://master:7077\", \"test\", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.NoClassDefFoundError\n",
       "Message: Could not initialize class com.datastax.driver.core.CodecRegistry\n",
       "StackTrace: com.datastax.driver.core.Configuration$Builder.build(Configuration.java:268)\n",
       "com.datastax.driver.core.Cluster$Builder.getConfiguration(Cluster.java:1232)\n",
       "com.datastax.driver.core.Cluster.<init>(Cluster.java:115)\n",
       "com.datastax.driver.core.Cluster.buildFrom(Cluster.java:182)\n",
       "com.datastax.driver.core.Cluster$Builder.build(Cluster.java:1249)\n",
       "com.datastax.spark.connector.cql.DefaultConnectionFactory$.createCluster(CassandraConnectionFactory.scala:85)\n",
       "com.datastax.spark.connector.cql.CassandraConnector$.com$datastax$spark$connector$cql$CassandraConnector$$createSession(CassandraConnector.scala:153)\n",
       "com.datastax.spark.connector.cql.CassandraConnector$$anonfun$2.apply(CassandraConnector.scala:148)\n",
       "com.datastax.spark.connector.cql.CassandraConnector$$anonfun$2.apply(CassandraConnector.scala:148)\n",
       "com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:31)\n",
       "com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:56)\n",
       "com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:81)\n",
       "com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:109)\n",
       "com.datastax.spark.connector.rdd.partitioner.CassandraRDDPartitioner$.getTokenFactory(CassandraRDDPartitioner.scala:176)\n",
       "org.apache.spark.sql.cassandra.CassandraSourceRelation$.apply(CassandraSourceRelation.scala:212)\n",
       "org.apache.spark.sql.cassandra.DefaultSource.createRelation(DefaultSource.scala:57)\n",
       "org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:125)\n",
       "org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:114)\n",
       "$line99.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:65)\n",
       "$line99.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:70)\n",
       "$line99.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:72)\n",
       "$line99.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:74)\n",
       "$line99.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:76)\n",
       "$line99.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:78)\n",
       "$line99.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:80)\n",
       "$line99.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:82)\n",
       "$line99.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:84)\n",
       "$line99.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:86)\n",
       "$line99.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:88)\n",
       "$line99.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:90)\n",
       "$line99.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:92)\n",
       "$line99.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:94)\n",
       "$line99.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:96)\n",
       "$line99.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:98)\n",
       "$line99.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:100)\n",
       "$line99.$read$$iwC$$iwC$$iwC.<init>(<console>:102)\n",
       "$line99.$read$$iwC$$iwC.<init>(<console>:104)\n",
       "$line99.$read$$iwC.<init>(<console>:106)\n",
       "$line99.$read.<init>(<console>:108)\n",
       "$line99.$read$.<init>(<console>:112)\n",
       "$line99.$read$.<clinit>(<console>)\n",
       "$line99.$eval$.<init>(<console>:7)\n",
       "$line99.$eval$.<clinit>(<console>)\n",
       "$line99.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:606)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:354)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:349)\n",
       "com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:348)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:348)\n",
       "com.ibm.spark.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:123)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "val df = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"dht11\", \"keyspace\" -> \"sensordata\")).load() // This DataFrame will use a spark.cassandra.input.size of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop\n",
    "import com.datastax.spark.connector._\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "val conf = new SparkConf(true).set(\"spark.cassandra.connection.host\", \"master\")\n",
    "val sc = new SparkContext(\"spark://master:7077\", \"test\", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.IllegalStateException\n",
       "Message: Detected Guava issue #1635 which indicates that a version of Guava less than 16.01 is in use.  This introduces codec resolution issues and potentially other incompatibility issues in the driver.  Please upgrade to Guava 16.01 or later.\n",
       "StackTrace: com.datastax.driver.core.SanityChecks.checkGuava(SanityChecks.java:62)\n",
       "com.datastax.driver.core.SanityChecks.check(SanityChecks.java:36)\n",
       "com.datastax.driver.core.Cluster.<clinit>(Cluster.java:67)\n",
       "com.datastax.spark.connector.cql.DefaultConnectionFactory$.clusterBuilder(CassandraConnectionFactory.scala:35)\n",
       "com.datastax.spark.connector.cql.DefaultConnectionFactory$.createCluster(CassandraConnectionFactory.scala:87)\n",
       "com.datastax.spark.connector.cql.CassandraConnector$.com$datastax$spark$connector$cql$CassandraConnector$$createSession(CassandraConnector.scala:153)\n",
       "com.datastax.spark.connector.cql.CassandraConnector$$anonfun$2.apply(CassandraConnector.scala:148)\n",
       "com.datastax.spark.connector.cql.CassandraConnector$$anonfun$2.apply(CassandraConnector.scala:148)\n",
       "com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:31)\n",
       "com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:56)\n",
       "com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:81)\n",
       "com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:109)\n",
       "com.datastax.spark.connector.rdd.partitioner.CassandraRDDPartitioner$.getTokenFactory(CassandraRDDPartitioner.scala:184)\n",
       "org.apache.spark.sql.cassandra.CassandraSourceRelation$.apply(CassandraSourceRelation.scala:212)\n",
       "org.apache.spark.sql.cassandra.DefaultSource.createRelation(DefaultSource.scala:57)\n",
       "org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:125)\n",
       "org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:114)\n",
       "$line28.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:29)\n",
       "$line28.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:34)\n",
       "$line28.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:36)\n",
       "$line28.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:38)\n",
       "$line28.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:40)\n",
       "$line28.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:42)\n",
       "$line28.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:44)\n",
       "$line28.$read$$iwC$$iwC$$iwC.<init>(<console>:46)\n",
       "$line28.$read$$iwC$$iwC.<init>(<console>:48)\n",
       "$line28.$read$$iwC.<init>(<console>:50)\n",
       "$line28.$read.<init>(<console>:52)\n",
       "$line28.$read$.<init>(<console>:56)\n",
       "$line28.$read$.<clinit>(<console>)\n",
       "$line28.$eval$.<init>(<console>:7)\n",
       "$line28.$eval$.<clinit>(<console>)\n",
       "$line28.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:606)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:354)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:349)\n",
       "com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:348)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:348)\n",
       "com.ibm.spark.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:123)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "val df = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"dht11\", \"keyspace\" -> \"sensordata\")).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.cassandra.CassandraSQLContext\n",
    "val cc = new CassandraSQLContext(sc)\n",
    "cc.setKeyspace(\"sensordata\")\n",
    "val dataframe = cc.sql(\"SELECT count(*) FROM mytable group by beamstamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = (sqlContext\n",
    "    .read\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\n",
    "    .options(table=\"dht11\", keyspace=\"sensordata\")\n",
    "    .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val rdd = sc.cassandraTable(\"test\", \"kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 2]"
     ]
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10 (Spark)",
   "language": "",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
