---
# If it's determined that "opcenter" will be the zeppelin host --> Ansible scripts from zepplin dev box


#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#!!!!!!!! MAKE SURE THAT PYTHON AND PIP ARE INSTALLED!!!!!!!!!
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


#- include: ruby.yml
#- include: nodejs.yml
#- include: maven.yml
#- include: python-addons.yml
#- include: r.yml #<-- Probably best to stick with the R scripts
#--------------------------------------------------------------------------------------------------------

# Configure the dependencies for Jupyter/Zeppelin IDE and the libraries for Data Science
- name: Create the Apps directory
  file: path=/home/{{ cluster_user }}/apps state=directory owner={{ cluster_user }} group={{ cluster_user }} mode=0777 recurse=yes

- name: Copy Spark-kernel to Apps directory
  copy: src=spark-kernel-0.1.5-SNAPSHOT.tar.gz dest=/home/{{ cluster_user }}/apps mode=0777

- name: Copy "uber" jar to Apps directory
  copy: src=spark-cassandra-connector-uber.jar dest=/home/{{ cluster_user }}/apps mode=0664

- name: Download Spark 1.6.1 to Apps directory
  get_url: url=http://apache.claz.org/spark/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-hadoop2.6.tgz dest=/home/{{ cluster_user }}/apps/spark-{{ spark_version }}1.6.1-bin-hadoop2.6.tgz

- name: Clone the latest version of Zeppelin
  git: repo=https://github.com/apache/incubator-zeppelin.git dest=/home/{{ cluster_user }}/apps

- name: Extract spark archive
  unarchive:
    src: apps/spark-{{ spark_version }}-bin-hadoop2.6.tgz
    dest: /home/{{ cluster_user }}/apps
    creates: /home/{{ cluster_user }}/apps/spark-{{ spark_version }}-bin-hadoop2.6
    copy: yes
    owner: "{{ cluster_user }}"
    group: "{{ cluster_user }}"

- name: Create Spark current version symbolic link and assign ownsership
  file: path=/home/{{ cluster_user }}/spark src=/home/{{ cluster_user }}/apps/spark-{{ spark_version }}-bin-hadoop2.6 state=link owner={{ cluster_user }} group={{ cluster_user }}

# The following may not be needed because the admin node is a spark driver, not a spark master
#- name: Set SPARK_HOME in .bashrc
#  lineinfile:
#    dest: '/home/{{ cluster_user }}/.bashrc'
#    line: 'export SPARK_HOME=/home/{{ cluster_user }}/apps/spark'
#    regexp: '^(# *)?export SPARK_HOME='
#  become: yes
#  become_user: "{{ cluster_user }}"

#- name: Add PATH to SPARK_HOME/bin in .bashrc
#  lineinfile:
#    dest: '/home/{{ cluster_user }}/.bashrc'
#    line: 'export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin # SPARK-BIN-PATH'
#    regexp: '# SPARK-BIN-PATH'
#  become: yes
#  become_user: "{{ cluster_user }}"

- name: Create scripts directory
  file: path=/home/{{ cluster_user }}/scripts state=directory owner={{ cluster_user }} group={{ cluster_user }} mode=0777 recurse=yes

- name: Copy deployment scripts
  template: src={{ item.src }} dest={{ item.dest }} owner={{ cluster_user }} group={{ cluster_user }} mode=0777
  with_items:
    - {src: "R.sh.j2", dest: "/home/{{ cluster_user }}/scripts/R.sh"}
    - {src: "jupyter.sh.j2", dest: "/home/{{ cluster_user }}/scripts/jupyter.sh"}
    - {src: "test_helper.sh.j2", dest: "/home/{{ cluster_user }}/scripts/test_helper.sh"}
    - {src: "rise.sh.j2", dest: "/home/{{ cluster_user }}/scripts/rise.sh"}
    - {src: "spark-kernel.sh.j2", dest: "/home/{{ cluster_user }}/scripts/spark-kernel.sh"}
    - {src: "irkernel.sh.j2", dest: "/home/{{ cluster_user }}/scripts/irkernel.sh"}
    - {src: "jupyter-extensions.sh.j2", dest: "/home/{{ cluster_user }}/scripts/jupyter-extensions.sh"}
    - {src: "rstudio-server.sh.j2", dest: "/home/{{ cluster_user }}/scripts/rstudio-server.sh"}
    - {src: "shiny-server.sh.j2", dest: "/home/{{ cluster_user }}/scripts/shiny-server.sh"}
    - {src: "zeppelin.sh.j2", dest: "/home/{{ cluster_user }}/scripts/zeppelin.sh"}
    - {src: "start-pyspark-notebook.sh.j2", dest: "/home/{{ cluster_user }}/scripts/start-pyspark-notebook.sh"}
    - {src: "sparkR-start.R.j2", dest: "/home/{{ cluster_user }}/scripts/sparkR-start.R"}
#    - {src: "zeppelin-env.sh.j2", dest: "/home/{{ cluster_user}}/apps/incubator-zeppelin/conf/zeppelin-env.sh"}
  
#- name: Install R (from scripts)
#  shell: /home/{{ cluster_user }}/scripts/R.sh
#  become: yes
#  become_user: "{{ cluster_user }}"
 
#- name: Install Jupyter (from scripts)
#  shell: /home/{{ cluster_user }}/scripts/jupyter.sh
#  become: yes
#  become_user: "{{ cluster_user }}"

#- name: Imstall test_helper (from scripts)
#  shell: /home/{{ cluster_user }}/scripts/test_helper.sh
#  become: yes
#  become_user: "{{ cluster_user }}"

#- name: Install Rise (from scripts)
#  shell: /home/{{ cluster_user }}/scripts/rise.sh
#  become: yes
#  become_user: "{{ cluster_user }}"

#- name: Install Spark-kernel (from scripts)
#  shell: /home/{{ cluster_user }}/scripts/spark-kernel.sh
#  become: yes
#  become_user: "{{ cluster_user }}"

#- name: Install irkernel (from scripts)
#  shell: /home/{{ cluster_user }}/scripts/irkernel.sh
#  become: yes
#  become_user: "{{ cluster_user }}"

#- name: Install Jupyter Extensions (from scripts)
#  shell: /home/{{ cluster_user }}/scripts/jupyter-extensions.sh
#  become: yes
#  become_user: "{{ cluster_user }}"
#  ignore_errors: yes

#- name: Install Rstudio Server (from scripts)
#  shell: /home/{{ cluster_user }}/scripts/rstudio-server.sh
#  become: yes
#  become_user: "{{ cluster_user }}"

#- name: Install Rstudio Shiny Server (from scripts)
#  shell: /home/{{ cluster_user }}/scripts/shiny-server.sh
#  become: yes
#  become_user: "{{ cluster_user }}"
