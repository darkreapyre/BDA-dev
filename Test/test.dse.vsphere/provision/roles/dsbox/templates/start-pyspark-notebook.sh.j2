#!/usr/bin/env bash

# The following is based on 4 Worker Virtual Machines with 16 Cores and 64GB RAM
MASTER="spark://{{ master }}"
NUM_EXECUTORS=2   # 24
EXECUTOR_CORES=16
EXECUTOR_MEMORY="60g"  # 10g
DRIVER_MEMORY="16g"

# ensure that always chmod go-wrx
umask 0077


DEFAULT_SUBMIT_ARGS="--packages com.databricks:spark-csv_2.10:1.4.0,datastax:spark-cassandra-connector:1.6.0-s_2.10 --driver-memory $DRIVER_MEMORY --num-executors $NUM_EXECUTORS --executor-cores $EXECUTOR_CORES --executor-memory $EXECUTOR_MEMORY"
PYSPARK_SUBMIT_ARGS="--master $MASTER $DEFAULT_SUBMIT_ARGS"

export PYSPARK_PYTHON=python
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS="notebook --ip='*' --port=8001 --no-browser" 

#echo $PYSPARK_SUBMIT_ARGS $DEFAULT_PYSPARK_SUBMIT_ARGS $PYSPARK_DRIVER_PYTHON $PYSPARK_DRIVER_PYTHON_OPTS
pyspark $PYSPARK_SUBMIT_ARGS
