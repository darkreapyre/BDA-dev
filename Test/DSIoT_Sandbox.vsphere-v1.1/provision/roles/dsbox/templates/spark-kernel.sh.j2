#!/usr/bin/env bash

HOME=/home/{{ cluster_user }}
SPARK_KERNEL_HOME=/opt
SPARK_KERNEL_FILE=spark-kernel-0.1.5-SNAPSHOT.tar.gz
CASSANDRA_KERNEL_FILE=kernel-0.1.4.5-cassandra.tgz
SCALA_KERNEL_PATH=$HOME/.local/share/jupyter/kernels/scala
CASSANDRA_KERNEL_PATH=$HOME/.local/share/jupyter/kernels/cassandra
ASSEMBLYJAR=$(echo $SPARK_KERNEL_HOME/kernel-0.1.4.5-cassandra/lib/kernel-assembly-*.jar)
TEMP_DIR=/tmp

if [ -z "$SPARK_HOME" ]; then
  export SPARK_HOME=/opt/spark
fi

# Extract the SPARK Kernel files
cd $SPARK_KERNEL_HOME
sudo cp /home/{{ cluster_user }}/apps/$SPARK_KERNEL_FILE .
sudo tar xzvf $SPARK_KERNEL_FILE
sudo chown -R root:root spark-kernel
sudo rm $SPARK_KERNEL_FILE
sudo -u {{ cluster_user }} mkdir -p $SCALA_KERNEL_PATH
sudo -u {{ cluster_user }} ln -s $SPARK_KERNEL_HOME/spark-kernel/logo-64x64.png $SCALA_KERNEL_PATH/logo-64x64.png

# Extract the Cassandra Kernel files
#cd $SPARK_KERNEL_HOME
#sudo cp /home/{{ cluster_user }}/apps/$CASSANDRA_KERNEL_FILE .
#sudo tar xzvf $CASSANDRA_KERNEL_FILE
#sudo chown -R root:root kernel-0.1.4.5-cassandra
#sudo rm $CASSANDRA_KERNEL_FILE
#sudo -u {{ cluster_user }} mkdir -p $CASSANDRA_KERNEL_PATH

# Create the Spark Kernel configuration
#the following kernel configuration is based on 4 Worker Virtual Machines with 16 Cores and 64GB RAM
sudo su {{ cluster_user }} -c 'cat > /home/{{ cluster_user }}/.local/share/jupyter/kernels/scala/kernel.json <<EOF
{
  "resources_dir": "/home/{{ cluster_user }}/.local/share/jupyter/kernels/scala/",
  "display_name": "Scala 2.10 (Spark)",
  "language_info": { "name": "scala" },
  "argv": [
    "/opt/spark-kernel/bin/spark-kernel",
    "--profile",
    "{connection_file}"
  ],
  "codemirror_mode": "scala",
  "env": {
    "SPARK_OPTS": "--master=spark://{{ groups['masters'][0] }}:7077 --packages=com.databricks:spark-csv_2.10:1.4.0 --driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=trace",
    "MAX_INTERPRETER_THREADS": "64",
    "SPARK_CONFIGURATION": "spark.cores.max=64",
    "CAPTURE_STANDARD_OUT": "true",
    "CAPTURE_STANDARD_ERR": "true",
    "SEND_EMPTY_OUTPUT": "false",
    "SPARK_HOME": "/opt/spark",
    "PYTHONPATH": "/opt/spark/python:/opt/spark/python/lib/py4j-0.8.2.1-src.zip"
 }
}
EOF'

# Create the Cassandra Kernel configuration
#sudo su {{ cluster_user }} -c 'cat > /home/{{ cluster_user }}/.local/share/jupyter/kernels/cassandra/kernel.json <<EOF
##{
#    "display_name": "Scala 2.10 (Cassandra)",
#    "language": "scala",
#    "argv": [
#        "ddc",
#        "spark-submit",
#        "--master",
#        "{{ groups['masters'][0] }}:7077",
#        "$ASSEMBLYJAR",
#        "com.ibm.spark.SparkKernel",
#        "--profile",
#        "{connection_file}"
#     ],
#     "codemirror_mode": "scala"
#}
#EOF'